# -*- coding: utf-8 -*-
"""ConsolidadoM8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FG4hKeuuKmRvUxkQsHM9iJdFiWoLS0YE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2

# 1Ô∏è Carga y exploraci√≥n de datos
def cargar_y_explorar(ruta):
    df = pd.read_csv(ruta)
    print("Resumen del dataset:")
    print(df.describe())

    # Eliminamos columna no num√©rica antes de calcular la correlaci√≥n
    df_numeric = df.drop(columns=["Pa√≠s"])

    print("\nCorrelaciones:")
    print(df_numeric.corr())

    # Visualizaci√≥n de correlaciones
    plt.figure(figsize=(10, 6))
    sns.heatmap(df_numeric.corr(), annot=True, cmap='coolwarm')
    plt.title("Matriz de correlaci√≥n")
    plt.tight_layout()
    plt.show()

    return df

# 2Ô∏è Preparaci√≥n de datos
def preparar_datos(df):
    df = df.drop(columns=["Pa√≠s"])  # Eliminamos columna no num√©rica
    X = df.drop(columns=["Tasa_Natalidad"])
    y = df["Tasa_Natalidad"]

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    return X_train, X_test, y_train, y_test, X.columns

# 3Ô∏è Construcci√≥n del modelo
def construir_modelo(input_dim, activacion='relu', dropout_rate=0.3, l2_rate=0.01):
    model = Sequential([
        Dense(64, activation=activacion, input_dim=input_dim, kernel_regularizer=l2(l2_rate)),
        Dropout(dropout_rate),
        Dense(32, activation=activacion, kernel_regularizer=l2(l2_rate)),
        Dropout(dropout_rate),
        Dense(1, activation='linear')  # Regresi√≥n
    ])
    return model

# 4Ô∏è Entrenamiento del modelo
def entrenar_modelo(model, X_train, y_train, lr=0.001, epochs=150):
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
                  loss='mean_squared_error',
                  metrics=['mae'])
    history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, verbose=0)
    return history

# 5Ô∏è Evaluaci√≥n del modelo
def evaluar_modelo(model, X_test, y_test):
    predicciones = model.predict(X_test).flatten()
    mae = mean_absolute_error(y_test, predicciones)
    rmse = np.sqrt(mean_squared_error(y_test, predicciones))
    print(f"MAE: {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")

    plt.scatter(y_test, predicciones, alpha=0.7)
    plt.xlabel("Tasa Natalidad Real")
    plt.ylabel("Tasa Natalidad Predicha")
    plt.title("Comparaci√≥n de predicciones")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return predicciones

# 6Ô∏è An√°lisis de importancia de variables
def importancia_variables(model, columnas):
    pesos = model.layers[0].get_weights()[0]
    importancia = np.sum(np.abs(pesos), axis=1)
    importancia_df = pd.Series(importancia, index=columnas).sort_values(ascending=False)

    importancia_df.plot(kind='bar', color='teal')
    plt.title("Importancia de variables en la predicci√≥n")
    plt.ylabel("Magnitud de peso absoluto")
    plt.tight_layout()
    plt.show()

    return importancia_df

# üîÅ Ejecuci√≥n principal
if __name__ == "__main__":
    ruta = "dataset_natalidad.csv"
    df = cargar_y_explorar(ruta)
    X_train, X_test, y_train, y_test, columnas = preparar_datos(df)
    modelo = construir_modelo(input_dim=X_train.shape[1])
    historia = entrenar_modelo(modelo, X_train, y_train)
    predicciones = evaluar_modelo(modelo, X_test, y_test)
    importancia = importancia_variables(modelo, columnas)

    # Guardar reflexiones clave
    print("\nVariables m√°s influyentes:")
    print(importancia.head(3))