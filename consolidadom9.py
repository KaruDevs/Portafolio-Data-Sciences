# -*- coding: utf-8 -*-
"""ConsolidadoM9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T2hY_Q2XLCAZH94UezVNG_ZXZYpLVmzc
"""

try:
    import google.colab
    !pip install -q pyspark
except:
    pass

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, desc
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Inicialización de Spark
spark = SparkSession.builder.appName("Migraciones").getOrCreate()

# Carga de datos
df = spark.read.csv("migraciones.csv", header=True, inferSchema=True)

# Rename the column 'Razón' to 'Razon' to avoid issues with Spark SQL
df = df.withColumnRenamed("Razón", "Razon")

rdd = df.rdd

# Exploración inicial
print("▶ Primeras filas:")
df.show(5)

print("▶ Esquema:")
df.printSchema()

print("▶ Estadísticas descriptivas:")
df.describe().show()

# Procesamiento con RDDs
print("▶ Migraciones económicas (RDD):")
rdd_economica = rdd.filter(lambda x: x.Razon == "Económica")
print(rdd_economica.collect())

print("▶ Países de origen:")
print(rdd.map(lambda x: x.Origen).take(3))

print("▶ Tokens de razones:")
print(rdd.flatMap(lambda x: x.Razon.split()).take(10))

print("▶ Total de registros:", rdd.count())

# Procesamiento con DataFrames
print("▶ Migraciones posteriores a 2016:")
df.filter(col("Año") > 2016).show()

print("▶ Promedio de PIB por destino:")
df.groupBy("Destino").agg(avg("PIB_Destino").alias("PIB_promedio")).show()

print("▶ Orden por desempleo en origen:")
df.orderBy(desc("Tasa_Desempleo_Origen")).show()

# Guardado en formato Parquet
df.write.mode("overwrite").parquet("migraciones_parquet")

# Consultas con Spark SQL
df.createOrReplaceTempView("migraciones")

print("▶ Principales países de origen:")
spark.sql("""
SELECT Origen, COUNT(*) AS Total
FROM migraciones
GROUP BY Origen
ORDER BY Total DESC
""").show()

print("▶ Principales países de destino:")
spark.sql("""
SELECT Destino, COUNT(*) AS Total
FROM migraciones
GROUP BY Destino
ORDER BY Total DESC
""").show()

print("▶ Razones de migración por región:")
spark.sql("""
SELECT Razon, COUNT(*) AS Frecuencia
FROM migraciones
GROUP BY Razon
ORDER BY Frecuencia DESC
""").show()

# Modelado con MLlib
df_ml = df.withColumn("label", (col("Razon") == "Económica").cast("integer"))

# Selección de variables predictoras
features = ["PIB_Origen", "PIB_Destino", "Tasa_Desempleo_Origen", "Tasa_Desempleo_Destino",
            "Nivel_Educativo_Origen", "Nivel_Educativo_Destino", "Población_Origen", "Población_Destino"]

assembler = VectorAssembler(inputCols=features, outputCol="features")
df_final = assembler.transform(df_ml).select("features", "label")

# Entrenamiento del modelo
lr = LogisticRegression()
modelo = lr.fit(df_final)

# Predicciones y evaluación
predicciones = modelo.transform(df_final)
evaluador = BinaryClassificationEvaluator()
precision = evaluador.evaluate(predicciones)

print(f"▶ Precisión del modelo de regresión logística: {precision:.2f}")